{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for Smash and Warhammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports for modeling\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#silence future warnings becuase they're annoying\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subreddits to use for model\n",
    "sub1 = 'Warhammer40k'\n",
    "sub2 = 'smashbros'\n",
    "sub3 = 'paintball'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data\n",
    "data1 = pd.read_csv(sub1+'.csv')\n",
    "data2 = pd.read_csv(sub2+'.csv')\n",
    "data3 = pd.read_csv(sub3+'.csv')\n",
    "\n",
    "#creating column indicating which sub a post is from\n",
    "data1['subreddit'] = 1\n",
    "data2['subreddit'] = 0\n",
    "data3['subreddit'] = 2\n",
    "df = pd.concat([data1,data2,data3])\n",
    "\n",
    "#defining features and target variable\n",
    "X = df['data']\n",
    "y = df['subreddit']\n",
    "\n",
    "#train test split to be used for all models\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "1. [Logistic Regression](#model1) Best Accuracy: 90.02% (TF-IDF)\n",
    "2. [KNN](#model2) Best Accuracy: 34.05% (TF-IDF)\n",
    "3. [Multinomial Naive Bayes](#model3) Best Accuracy: 90.02% (TF-IDF)\n",
    "\n",
    "### Hyperparamter Tuning\n",
    "Most hyperparamters were found by experimentation in gridsearch using bisection search algorithm. This method assumes that effects of a given hyperparameter are linear, which may not always be the case, but does always converge to a local optimizer regardless  \n",
    "\n",
    "reference: https://en.wikipedia.org/wiki/Bisection_method\n",
    "\n",
    "In essence, \n",
    "- test between 2 values, that are arbitrary percieved bounds\n",
    "- keep the one that gives a higher score and replace the other with the average of the 2 values\n",
    "- test between previous best and average as new values\n",
    "- repeat until convergence\n",
    "\n",
    "To make the model more robust I tested 3 values at each iteration, due to uncertainity about effects of each parameter on the model: low, high, and mean\n",
    "\n",
    "(Didn't go into the 50's or decimals, so stopped algorithm early to keep things even)\n",
    "\n",
    "### Using TF-IDF with Multinomial\n",
    "\n",
    "Justification for using TF-IDF with multinomial naive bayes:\n",
    "‚ÄúThe multinomial Naive Bayes classifier is suitable for classification with¬†discrete features¬†(e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.‚Äù https://www.ritchieng.com/machine-learning-multinomial-naive-bayes-vectorization/![image.png](attachment:image.png)\n",
    "\n",
    "Using Guassian Naive Bayes requires a full matrix as opposed to a sparse matrix and the conversion from the vectorizer output to a dataframe was too computationally expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Pedram\n",
    "custom = text.ENGLISH_STOP_WORDS.union(['amp', 'new', 'like', 'got','know','just', 've','don', 'think'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Logistic Regression\n",
    "<a id='model1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('lr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'vec': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vec__stop_words': ['english',custom]\n",
    "    'vec__max_features': [2500,3000, 3500], #tried [3000, 3100, 3200,]\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [.85],\n",
    "    'vec__ngram_range': [(1,1),(1,2)],\n",
    "    'lr__C': [1,5], #tried [1,5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvs: 0.8802502234137622\n",
      "train score: 0.9705093833780161\n",
      "test score: 0.8994638069705094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 1,\n",
       " 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=0.85, max_features=3500,\n",
       "                 min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                       'afterwards', 'again', 'against', 'all',\n",
       "                                       'almost', 'alone', 'along', 'already',\n",
       "                                       'also', 'although', 'always', 'am',\n",
       "                                       'among', 'amongst', 'amoungst', 'amount',\n",
       "                                       'amp', 'an', 'and', 'another', 'any',\n",
       "                                       'anyhow', 'anyone', 'anything', 'anyway', ...}),\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                 vocabulary=None),\n",
       " 'vec__max_df': 0.85,\n",
       " 'vec__max_features': 3500,\n",
       " 'vec__min_df': 2,\n",
       " 'vec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print('cvs:', gs.best_score_)\n",
    "print('train score:', gs.score(X_train, y_train))\n",
    "print('test score:', gs.score(X_test, y_test))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score on testing set was 90.03% accuracy, using custom stop words. Changing stop words cause less improvement with this model than naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 2: k-Nearest Neighbors\n",
    "<a id='model2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', CountVectorizer(stop_words='english')),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'vec': [CountVectorizer(stop_words='english'), TfidfVectorizer(stop_words='english')],\n",
    "    'vec__max_features': [3000, 3100],\n",
    "    'vec__min_df': [2],\n",
    "    'vec__max_df': [.9],\n",
    "    'vec__ngram_range': [(1,1),(1,2)],\n",
    "    'knn__n_neighbors': [35]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvs: 0.7479892761394102\n",
      "train score: 0.34405719392314565\n",
      "test score: 0.34048257372654156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'knn__n_neighbors': 35,\n",
       " 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=0.9, max_features=3100,\n",
       "                 min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                 sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                 tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'vec__max_df': 0.9,\n",
       " 'vec__max_features': 3100,\n",
       " 'vec__min_df': 2,\n",
       " 'vec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print('cvs:', gs.best_score_)\n",
    "print('train score:', gs.score(X_train, y_train))\n",
    "print('test score:', gs.score(X_test, y_test))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty terrible in comparison to the other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 3: Multinomial Naive Bayes\n",
    "<a id='model3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Guassian is too computationally expensive as it requires a dense matrix not a sparse one. Attempting multinomial to see what happens as the matrix is sparse anyways it violates fewer assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', CountVectorizer()),\n",
    "    ('nb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_params = {\n",
    "    'vec': [CountVectorizer(), TfidfVectorizer()],\n",
    "    'vec__stop_words': [custom, 'english'],\n",
    "    'vec__max_features': [4200],\n",
    "    'vec__min_df': [1, 2],\n",
    "    'vec__max_df': [.9],\n",
    "    'vec__ngram_range': [(1,1)],\n",
    "    'nb__alpha': [1,3,5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cvs: 0.8927613941018767\n",
      "train score: 0.9700625558534406\n",
      "test score: 0.9075067024128687\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'nb__alpha': 1,\n",
       " 'vec': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                 dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                 input='content', lowercase=True, max_df=0.9, max_features=4200,\n",
       "                 min_df=2, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                 smooth_idf=True,\n",
       "                 stop_words=frozenset({'a', 'about', 'above', 'across', 'after',\n",
       "                                       'afterwards', 'again', 'against', 'all',\n",
       "                                       'almost', 'alone', 'along', 'already',\n",
       "                                       'also', 'although', 'always', 'am',\n",
       "                                       'among', 'amongst', 'amoungst', 'amount',\n",
       "                                       'amp', 'an', 'and', 'another', 'any',\n",
       "                                       'anyhow', 'anyone', 'anything', 'anyway', ...}),\n",
       "                 strip_accents=None, sublinear_tf=False,\n",
       "                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                 vocabulary=None),\n",
       " 'vec__max_df': 0.9,\n",
       " 'vec__max_features': 4200,\n",
       " 'vec__min_df': 2,\n",
       " 'vec__ngram_range': (1, 1),\n",
       " 'vec__stop_words': frozenset({'a',\n",
       "            'about',\n",
       "            'above',\n",
       "            'across',\n",
       "            'after',\n",
       "            'afterwards',\n",
       "            'again',\n",
       "            'against',\n",
       "            'all',\n",
       "            'almost',\n",
       "            'alone',\n",
       "            'along',\n",
       "            'already',\n",
       "            'also',\n",
       "            'although',\n",
       "            'always',\n",
       "            'am',\n",
       "            'among',\n",
       "            'amongst',\n",
       "            'amoungst',\n",
       "            'amount',\n",
       "            'amp',\n",
       "            'an',\n",
       "            'and',\n",
       "            'another',\n",
       "            'any',\n",
       "            'anyhow',\n",
       "            'anyone',\n",
       "            'anything',\n",
       "            'anyway',\n",
       "            'anywhere',\n",
       "            'are',\n",
       "            'around',\n",
       "            'as',\n",
       "            'at',\n",
       "            'back',\n",
       "            'be',\n",
       "            'became',\n",
       "            'because',\n",
       "            'become',\n",
       "            'becomes',\n",
       "            'becoming',\n",
       "            'been',\n",
       "            'before',\n",
       "            'beforehand',\n",
       "            'behind',\n",
       "            'being',\n",
       "            'below',\n",
       "            'beside',\n",
       "            'besides',\n",
       "            'between',\n",
       "            'beyond',\n",
       "            'bill',\n",
       "            'both',\n",
       "            'bottom',\n",
       "            'but',\n",
       "            'by',\n",
       "            'call',\n",
       "            'can',\n",
       "            'cannot',\n",
       "            'cant',\n",
       "            'co',\n",
       "            'con',\n",
       "            'could',\n",
       "            'couldnt',\n",
       "            'cry',\n",
       "            'de',\n",
       "            'describe',\n",
       "            'detail',\n",
       "            'do',\n",
       "            'don',\n",
       "            'done',\n",
       "            'down',\n",
       "            'due',\n",
       "            'during',\n",
       "            'each',\n",
       "            'eg',\n",
       "            'eight',\n",
       "            'either',\n",
       "            'eleven',\n",
       "            'else',\n",
       "            'elsewhere',\n",
       "            'empty',\n",
       "            'enough',\n",
       "            'etc',\n",
       "            'even',\n",
       "            'ever',\n",
       "            'every',\n",
       "            'everyone',\n",
       "            'everything',\n",
       "            'everywhere',\n",
       "            'except',\n",
       "            'few',\n",
       "            'fifteen',\n",
       "            'fifty',\n",
       "            'fill',\n",
       "            'find',\n",
       "            'fire',\n",
       "            'first',\n",
       "            'five',\n",
       "            'for',\n",
       "            'former',\n",
       "            'formerly',\n",
       "            'forty',\n",
       "            'found',\n",
       "            'four',\n",
       "            'from',\n",
       "            'front',\n",
       "            'full',\n",
       "            'further',\n",
       "            'get',\n",
       "            'give',\n",
       "            'go',\n",
       "            'got',\n",
       "            'had',\n",
       "            'has',\n",
       "            'hasnt',\n",
       "            'have',\n",
       "            'he',\n",
       "            'hence',\n",
       "            'her',\n",
       "            'here',\n",
       "            'hereafter',\n",
       "            'hereby',\n",
       "            'herein',\n",
       "            'hereupon',\n",
       "            'hers',\n",
       "            'herself',\n",
       "            'him',\n",
       "            'himself',\n",
       "            'his',\n",
       "            'how',\n",
       "            'however',\n",
       "            'hundred',\n",
       "            'i',\n",
       "            'ie',\n",
       "            'if',\n",
       "            'in',\n",
       "            'inc',\n",
       "            'indeed',\n",
       "            'interest',\n",
       "            'into',\n",
       "            'is',\n",
       "            'it',\n",
       "            'its',\n",
       "            'itself',\n",
       "            'just',\n",
       "            'keep',\n",
       "            'know',\n",
       "            'last',\n",
       "            'latter',\n",
       "            'latterly',\n",
       "            'least',\n",
       "            'less',\n",
       "            'like',\n",
       "            'ltd',\n",
       "            'made',\n",
       "            'many',\n",
       "            'may',\n",
       "            'me',\n",
       "            'meanwhile',\n",
       "            'might',\n",
       "            'mill',\n",
       "            'mine',\n",
       "            'more',\n",
       "            'moreover',\n",
       "            'most',\n",
       "            'mostly',\n",
       "            'move',\n",
       "            'much',\n",
       "            'must',\n",
       "            'my',\n",
       "            'myself',\n",
       "            'name',\n",
       "            'namely',\n",
       "            'neither',\n",
       "            'never',\n",
       "            'nevertheless',\n",
       "            'new',\n",
       "            'next',\n",
       "            'nine',\n",
       "            'no',\n",
       "            'nobody',\n",
       "            'none',\n",
       "            'noone',\n",
       "            'nor',\n",
       "            'not',\n",
       "            'nothing',\n",
       "            'now',\n",
       "            'nowhere',\n",
       "            'of',\n",
       "            'off',\n",
       "            'often',\n",
       "            'on',\n",
       "            'once',\n",
       "            'one',\n",
       "            'only',\n",
       "            'onto',\n",
       "            'or',\n",
       "            'other',\n",
       "            'others',\n",
       "            'otherwise',\n",
       "            'our',\n",
       "            'ours',\n",
       "            'ourselves',\n",
       "            'out',\n",
       "            'over',\n",
       "            'own',\n",
       "            'part',\n",
       "            'per',\n",
       "            'perhaps',\n",
       "            'please',\n",
       "            'put',\n",
       "            'rather',\n",
       "            're',\n",
       "            'same',\n",
       "            'see',\n",
       "            'seem',\n",
       "            'seemed',\n",
       "            'seeming',\n",
       "            'seems',\n",
       "            'serious',\n",
       "            'several',\n",
       "            'she',\n",
       "            'should',\n",
       "            'show',\n",
       "            'side',\n",
       "            'since',\n",
       "            'sincere',\n",
       "            'six',\n",
       "            'sixty',\n",
       "            'so',\n",
       "            'some',\n",
       "            'somehow',\n",
       "            'someone',\n",
       "            'something',\n",
       "            'sometime',\n",
       "            'sometimes',\n",
       "            'somewhere',\n",
       "            'still',\n",
       "            'such',\n",
       "            'system',\n",
       "            'take',\n",
       "            'ten',\n",
       "            'than',\n",
       "            'that',\n",
       "            'the',\n",
       "            'their',\n",
       "            'them',\n",
       "            'themselves',\n",
       "            'then',\n",
       "            'thence',\n",
       "            'there',\n",
       "            'thereafter',\n",
       "            'thereby',\n",
       "            'therefore',\n",
       "            'therein',\n",
       "            'thereupon',\n",
       "            'these',\n",
       "            'they',\n",
       "            'thick',\n",
       "            'thin',\n",
       "            'think',\n",
       "            'third',\n",
       "            'this',\n",
       "            'those',\n",
       "            'though',\n",
       "            'three',\n",
       "            'through',\n",
       "            'throughout',\n",
       "            'thru',\n",
       "            'thus',\n",
       "            'to',\n",
       "            'together',\n",
       "            'too',\n",
       "            'top',\n",
       "            'toward',\n",
       "            'towards',\n",
       "            'twelve',\n",
       "            'twenty',\n",
       "            'two',\n",
       "            'un',\n",
       "            'under',\n",
       "            'until',\n",
       "            'up',\n",
       "            'upon',\n",
       "            'us',\n",
       "            've',\n",
       "            'very',\n",
       "            'via',\n",
       "            'was',\n",
       "            'we',\n",
       "            'well',\n",
       "            'were',\n",
       "            'what',\n",
       "            'whatever',\n",
       "            'when',\n",
       "            'whence',\n",
       "            'whenever',\n",
       "            'where',\n",
       "            'whereafter',\n",
       "            'whereas',\n",
       "            'whereby',\n",
       "            'wherein',\n",
       "            'whereupon',\n",
       "            'wherever',\n",
       "            'whether',\n",
       "            'which',\n",
       "            'while',\n",
       "            'whither',\n",
       "            'who',\n",
       "            'whoever',\n",
       "            'whole',\n",
       "            'whom',\n",
       "            'whose',\n",
       "            'why',\n",
       "            'will',\n",
       "            'with',\n",
       "            'within',\n",
       "            'without',\n",
       "            'would',\n",
       "            'yet',\n",
       "            'you',\n",
       "            'your',\n",
       "            'yours',\n",
       "            'yourself',\n",
       "            'yourselves'})}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, param_grid=pipe_params, cv=3)\n",
    "gs.fit(X_train, y_train)\n",
    "print('cvs:', gs.best_score_)\n",
    "print('train score:', gs.score(X_train, y_train))\n",
    "print('test score:', gs.score(X_test, y_test))\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score after adjusting stop words was 90.75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization\n",
    "Attempting to use bayesian hyper paramter optimization to improve score further with less guessing and checking.  \n",
    "https://en.wikipedia.org/wiki/Hyperparameter_optimization#Bayesian_optimization\n",
    "\n",
    "Using this guide: https://www.districtdatalabs.com/parameter-tuning-with-hyperopt\n",
    "\n",
    "Didn't exactly work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [06:04<00:00,  1.75s/it, best loss: -0.9039269734739535]\n",
      "{'nb_alpha': 67, 'vec__min_df': 2, 'vec_max_features': 1918}\n"
     ]
    }
   ],
   "source": [
    "def test(params):\n",
    "    pipe = Pipeline([\n",
    "        ('vec', TfidfVectorizer(stop_words='english',max_df=.9, ngram_range=(1,1))),\n",
    "        ('nb', MultinomialNB())\n",
    "        ])\n",
    "    return cross_val_score(pipe, X_train, y_train, cv=5).mean()\n",
    "\n",
    "space = {\n",
    "    'vec__max_features': hp.choice('vec_max_features',range(2000, 8000)),\n",
    "    'vec__min_df': hp.choice('vec__min_df', range(1,4)),\n",
    "    'nb_alpha': hp.choice('nb_alpha', range(1,80))\n",
    "}\n",
    "\n",
    "def f(params):\n",
    "    acc = test(params)\n",
    "    return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(f, space, algo=tpe.suggest, max_evals=300, trials=trials)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score: 0.8480786416443253\n",
      "test score: 0.8378016085790885\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vec', CountVectorizer(stop_words=custom, max_df=.9, min_df=2, ngram_range=(1,1), max_features= 1918)),\n",
    "    ('nb', MultinomialNB(alpha=67))\n",
    "])\n",
    "pipe.fit(X_train, y_train)\n",
    "print('train score:', pipe.score(X_train, y_train))\n",
    "print('test score:', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several attempts didn't really seem help the model at all, so back to guess and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting the model\n",
    "Looking at coefficients to see which words are the most indicative of a post belonging in a certain subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words=custom, max_features=4000, min_df=2, max_df=.9, ngram_range=(1,1))\n",
    "tf.fit(X_train)\n",
    "X_df = tf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>smash</td>\n",
       "      <td>-5.093695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>https</td>\n",
       "      <td>-5.620131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3705</th>\n",
       "      <td>ultimate</td>\n",
       "      <td>-5.709280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>com</td>\n",
       "      <td>-5.798964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>game</td>\n",
       "      <td>-6.067114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>bros</td>\n",
       "      <td>-6.159111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>character</td>\n",
       "      <td>-6.220956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>tournament</td>\n",
       "      <td>-6.274434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        features      prob\n",
       "3200       smash -5.093695\n",
       "1732       https -5.620131\n",
       "3705    ultimate -5.709280\n",
       "787          com -5.798964\n",
       "1487        game -6.067114\n",
       "587         bros -6.159111\n",
       "702    character -6.220956\n",
       "3622  tournament -6.274434"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smash = pd.DataFrame()\n",
    "smash['features'] = tf.get_feature_names()\n",
    "smash['prob'] = nb.feature_log_prob_[0]\n",
    "smash.sort_values(by='prob',ascending=False, inplace=True)\n",
    "smash.head(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>finished</td>\n",
       "      <td>-5.744091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>army</td>\n",
       "      <td>-5.782549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3886</th>\n",
       "      <td>welcome</td>\n",
       "      <td>-5.942247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3913</th>\n",
       "      <td>wip</td>\n",
       "      <td>-5.948089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>chaos</td>\n",
       "      <td>-5.988201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>contrast</td>\n",
       "      <td>-6.072290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2693</th>\n",
       "      <td>primaris</td>\n",
       "      <td>-6.077437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2492</th>\n",
       "      <td>paint</td>\n",
       "      <td>-6.113726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      features      prob\n",
       "1385  finished -5.744091\n",
       "380       army -5.782549\n",
       "3886   welcome -5.942247\n",
       "3913       wip -5.948089\n",
       "698      chaos -5.988201\n",
       "865   contrast -6.072290\n",
       "2693  primaris -6.077437\n",
       "2492     paint -6.113726"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w40k = pd.DataFrame()\n",
    "w40k['features'] = tf.get_feature_names()\n",
    "w40k['prob'] = nb.feature_log_prob_[1]\n",
    "w40k.sort_values(by='prob',ascending=False, inplace=True)\n",
    "w40k.head(n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2493</th>\n",
       "      <td>paintball</td>\n",
       "      <td>-5.191856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605</th>\n",
       "      <td>gun</td>\n",
       "      <td>-5.785647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3478</th>\n",
       "      <td>tank</td>\n",
       "      <td>-5.946430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>marker</td>\n",
       "      <td>-5.949233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>looking</td>\n",
       "      <td>-6.027609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1553</th>\n",
       "      <td>good</td>\n",
       "      <td>-6.267589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>barrel</td>\n",
       "      <td>-6.280737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>dye</td>\n",
       "      <td>-6.321951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       features      prob\n",
       "2493  paintball -5.191856\n",
       "1605        gun -5.785647\n",
       "3478       tank -5.946430\n",
       "2169     marker -5.949233\n",
       "2084    looking -6.027609\n",
       "1553       good -6.267589\n",
       "461      barrel -6.280737\n",
       "1157        dye -6.321951"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paint = pd.DataFrame()\n",
    "paint['features'] = tf.get_feature_names()\n",
    "paint['prob'] = nb.feature_log_prob_[2]\n",
    "paint.sort_values(by='prob',ascending=False, inplace=True)\n",
    "paint.head(n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posts that were predicted wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(stop_words=custom, max_features=4000, min_df=2, max_df=.9, ngram_range=(1,1))\n",
    "tf.fit(X_train)\n",
    "X_df = tf.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unk = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nb.predict(X_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = pd.DataFrame(X_test[y_test != pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freeflow Caliber Minis- July 5th only: \n",
      "Help identifying this please: My best friend passed away from cancer in october and I took some vacation time to help clean up his room. He left this tank to me in his will and I‚Äôm not sure what game system it‚Äôs from. It‚Äôs in a hovering base and it‚Äôs definitely made of metal. Any help would be appreciated.https://imgur.com/gallery/N2wMIRq\n",
      "Scout tank, ready to outflank!: \n",
      "I'm pretty sure this is the fastest comeback i've ever made: \n",
      "217    After a hiatus (school, work, laziness), here ...\n",
      "217         Tetanus shots , tetanus shots for everyone: \n",
      "Name: data, dtype: object\n",
      "First video and 3rd time playing! Ready to play more!: \n",
      "217    After a hiatus (school, work, laziness), here ...\n",
      "217         Tetanus shots , tetanus shots for everyone: \n",
      "Name: data, dtype: object\n",
      "PSI Shinespike: \n",
      "Worth it to put havoks in a rhino?: \n",
      "Wall off retirement: \n",
      "Installed dual Fans on my CMD: \n",
      "What's this piece of art called, and where can I find the full thing digitally?: \n",
      "\"Who needs wobbling?\": \n",
      "Agg trends: What are your favorite past or present \"agg\" trends?\n",
      "Just came in the mail today: \n",
      "Thoughts on Dangerous Power E2?: \n",
      "We all connected to the holy Wi-Fi: \n",
      "768    Still fairly new, but getting better: \n",
      "768           He should have shut his mouth: \n",
      "Name: data, dtype: object\n",
      "We are stronger together b≈ï√∏≈•hƒô≈ô: \n",
      "Help me decide LPR and Barrel colors - [r/autococker101]: Working on a build and talking about it here:  [https://www.reddit.com/r/Autocockers101/comments/bqjc8t/building\\_my\\_first\\_cocker\\_a\\_few\\_questions\\_please/](https://www.reddit.com/r/Autocockers101/comments/bqjc8t/building_my_first_cocker_a_few_questions_please/) Having a hard time deciding on this last part for the front pneus.  Possibly barrel color too. I thought I wanted to go dust silver at first to match the fade.Help me decide? I've got 2 sets I've mocked up as I'm also deciding between barrel color as well. \\*Not everything is exact, just to give an idea.* Several different Rock LPRs (Chrome, Purple Gloss Anon, Brass) with Silver Barrels - [https://imgur.com/0Qz9XmB](https://imgur.com/0Qz9XmB)* Several different Rock LPRs (Chrome, Purple Gloss Anon, Brass) with Black Barrels - [https://imgur.com/wVZhUCm](https://imgur.com/wVZhUCm)Here is a picture I've where I'm at so far: [https://imgur.com/a/smREXo3](https://imgur.com/a/smREXo3)\n",
      "Too much gas: \n",
      "Bringing a Classic back to life. +Cat: \n",
      "Purchased my first true high end and I want to die. Having adult money rules.: \n",
      "My first ever mini (I wish I could post more than one photo somehow)! C&amp;C desperately needed: \n",
      "Two of them: \n",
      "My 7 year olds way more patient than me at his age , shame about the his lack of taste though ....üòÇ: \n",
      "A 10+ Year Journey: The Rise of Axe: \n",
      "Does anybody who knows french know what happened in gluttony's entrance at top 8?: I want to get the joke\n",
      "Looks pretty normal to me üëå: \n",
      "Who needs deep breathing anyway?: \n",
      "Il 360 NoScope your MOM!: \n",
      "Still available!: \n",
      "768    Still fairly new, but getting better: \n",
      "768           He should have shut his mouth: \n",
      "Name: data, dtype: object\n",
      "When you hate 9v batteries with a burning passion, you do silly things..: \n",
      "Decided to join the club: \n",
      "Any idea if we‚Äôre getting a breakdown video for the Hero?: Title basically. Ever since his announcement I‚Äôve been really confused on how switching between spells for each special is going to work. I would love a breakdown video before he‚Äôs released I‚Äôm just wondering if anyone knows if they‚Äôre planning one.\n",
      "Jumpshot: Thought you all might appreciate this beautiful move:   [https://media.giphy.com/media/lmiejiQ2SbWghYgqfU/giphy.gif](https://media.giphy.com/media/lmiejiQ2SbWghYgqfU/giphy.gif)\n",
      "Time to get back in.: \n",
      "Getting a Defy Conquest any advice?: \n",
      "Reading the liber chaotica?: I want to read it but I also am not looking to spend 200 on a book. I saw the rerelease but I forgot to buy it and now they‚Äôre going for a ton of money. What is my best option now that won‚Äôt cost a ton of money?\n",
      "Our gun post on Omaha beach after fierce combat. Numerous 37mm hits are well visible down there: \n",
      "Fail Shooting Video: Breaking paint. https://reddit.com/link/br4y96/video/x7um6gvh8hz21/player\n",
      "Tiger in the grass: \n",
      "This video is criminally under-viewed. Play with sound.: \n",
      "An honest question.: Hey all. I‚Äôm a 16 year old Asian kid. My friends and I really want to go to this one local because we don‚Äôt have to rely on our parents since I can drive now. However, we are being hesitant due to recent news. One of my friends have been to a local before and he was made fun of for his race and he said it smelled bad. So the honest question is, are these standards everywhere?\n",
      "I don't normally paint interiors but I'm giving it a try this time: \n",
      "Spring 2019 FAQ is up: \n",
      "My unpopular opinion: many players seem almost afraid of making their list \"too competitive\" for fear of being negatively labelled as \"win at all costs\". People should be building their armies with more teeth.: Communication before a game is ideal. If I know my oponent is going to be bringing a less competitive and more fluffy list then I will try to do the same and I've had some great fun with this. But it gets a bit old and I have noticed a theme of players announcing their list is \"not competitive\" or worse \"only semi-competitive\" in local tournaments almost as an excuse for losing before games even begin. As an oponent this sucks because steam rolling someone who knowingly avoids their faction's best units is pointless. Build lists with teeth and have more fun?\n",
      "I feel better already!: &amp;#x200B;https://i.redd.it/lr44oa51ujz21.jpg\n",
      "My Vigilator boys: \n",
      "Sometimes the empty bunkers are empty for a reason....: \n",
      "Can anyone tell me how to put this together or at least what model it is so I can find the manual??: \n",
      "Top 10 pgr predictions: With the season ending next week who do you have finishing in the top 10,my guesses would be1.Mkleo2. Tweek3.Marss4.Samsora5.Ally6. Dabuz7.Void8.Nairo9.Cosmos10.Shuton\n",
      "Another Big Game in the books: \n",
      "Massachusetts tourneys like synthesis: Synthesis is far away and I‚Äôm like are they any of these caliber nearby\n",
      "Magnetised some deff koptas. Rokkits, kmb and twin big shootas (or in this case, quad-shootas) at my disposal. Next are the bigbomms...: \n",
      "Ever wonder what happens when a fully charged waft and darkest lariat collide? Here you go: \n",
      "Behold, a lad: \n",
      "Twister goodness: \n",
      "WDP Wednesday?: \n",
      "Is it too much to ask for a Tartakovsky style animated show of this? What has been the general consensus of these books?: \n",
      "The snipe: \n",
      "After some feedback I stripped and repainted her face. Looks better but still got a lot to learn.: \n",
      "Ah Shit, Here We Go Again: \n",
      "The Decline of Spacies: \n",
      "It would be wierd if, like, 10000 years after everyone do this...: \n",
      "E Town Beat Down!: \n",
      "If I‚Äôm going down, you‚Äôre coming too!: \n",
      "I had no idea Leo loved big chungus so much: \n",
      "Hello I‚Äôm new: \n"
     ]
    }
   ],
   "source": [
    "for i in wrong.index:\n",
    "    print(wrong['data'][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
